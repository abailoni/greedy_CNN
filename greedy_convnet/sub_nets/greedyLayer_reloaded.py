__author__ = "abailoni"

'''
Network that will finetune the pre-trained perceptrons of the currently trained layer.
'''

import numpy as np
from copy import deepcopy
import json

import theano.tensor as T
from lasagne import layers
from lasagne.nonlinearities import rectify, identity

from lasagne.init import Normal

import mod_nolearn.segm.segm_utils as segm_utils
from mod_nolearn.segm import segmNeuralNet
from greedy_convnet import BatchIterator_Greedy
from various.utils import join_dict

class greedyLayer_reload(object):
    '''
    # ---------------------------------------------
    # STRUCTURE OF THE CLASS: (main attributes)
    # ---------------------------------------------

        - net:
            TYPE: istance of NeuralNet() in mod_nolearn
            VALUE: attribute containing the actual greedy network that will be trained

        - num_classes
            TYPE: int

        - batch_size:
            TYPE: int

        - batchShuffle
            TYPE: boolean

        - eval_size
            TYPE: float
            VALUE: proportion in [0,1] between validation and training data
                E.g. eval_size=0.2 gives 20% of validation and 80% of training data

        - init_weight:
            TYPE: float
            VALUE: standard deviation used for initialization of the weights in the trained layer

        - filter_size
            TYPE: int

        - num_filters
            TYPE: int

        - nodes_partition
            TYPE: list of ints
            VALUES: a[i] contains the number of the starting filter in the i-th perceptron

        - active_perceptrons
            TYPE: int
            VALUE: how many perceptron are active and are then finetuned by calling the fit fucntion self.net.fit()

        - active_nodes
            TYPE: int


    '''
    def __init__(
            self,
            fixed_input_layers,
            layers_info,
            name_trained_layer,
            layer_kwargs,
            FULLNET_kwargs,
            list_boost_filters,
            fixed_weights,
            **greedy_kwargs):
        '''
        # ----------
        # INPUTS:
        # ----------

            - fixed_input_layers:
                TYPE: dictionary
                VALUE: nolearn-dictionary with previous layers that won't be trained

            - fixed_weights:
                TYPE: dictionary
                VALUES: weights for the fixed layers (check attribute trained_weights in class greedyNet)

            - layers_info:
                TYPE: dictionary
                VALUE: infos about all layers (e.g. output_shape, req, trained_greedily, trained, type...)

            - name_trained_layer
                TYPE: string
                VALUE: name of the trained layer (mainly for log and to check infos)

            - layer_kwargs:
                TYPE: dictionary
                VALUES: nolearn parameters passed to the trained layer in the original complete network

            - list_boost_filters:
                TYPE: list
                VALUES: number of filters for each perceptron (e.g. [5,5,5,5,5,3])

            - FULLNET_kwargs:
                TYPE: dictionary
                VALUES: arguments valid for every trained layer (e.g. num_classes, batch_size...).
                    Most of them are passed to the initialization of self.net

            - greedy_kwargs:
                TYPE: additional parameters
                VALUES: additional parameters specific for this layer, plus some automatically generated by init_greedyLayer (e.g. logs_path...). Usually here there are parameters like learning_rate, etc...
                    Most of them are passed to the initialization of self.net
        '''
        # info = deepcopy(greedy_kwargs)
        self.fixed_input_layers = fixed_input_layers

        # -----------------
        # Attributes from full NET:
        # -----------------
        self.num_classes = FULLNET_kwargs.pop('num_classes', 2)
        self.batch_size = FULLNET_kwargs.pop('batch_size', 20)
        self.batchShuffle = FULLNET_kwargs.pop('batchShuffle', True)
        self.eval_size = FULLNET_kwargs.pop('eval_size', 0.2)

        FULLNET_init_weight = FULLNET_kwargs.pop('init_weight', 1e-3)
        self.init_weight = greedy_kwargs.pop('init_weight', FULLNET_init_weight)
        # NOT SURE ABOUT THIS INIT. METHOD
        if 'W' not in layer_kwargs:
            layer_kwargs['W'] = Normal(std=self.init_weight)

        # -----------------
        # Layer-specific:
        # -----------------
        self.layer_output_shape = layers_info[name_trained_layer]['output_shape']
        try:
            self.filter_size = layer_kwargs['filter_size']
            self.num_filters = layer_kwargs['num_filters']
        except KeyError:
            raise KeyError("For layers trained greedily (conv and transConv) a filter_size and num_filters is required")

        # -----------------
        # Are there other not used parameters?
        # -----------------
        self.FULLNET_kwargs = FULLNET_kwargs
        if FULLNET_kwargs:
            greedy_kwargs = join_dict(FULLNET_kwargs, greedy_kwargs)

        # ---------------------------------------------------
        # Specific manual-inputed attributes for greedy-training:
        # ---------------------------------------------------
        greedy_kwargs['name'] = "greedy_"+name_trained_layer

        self.active_perceptrons = 0
        self.active_nodes = 0

        # Checks:
        if "train_split" in greedy_kwargs:
            raise ValueError('The option train_split is not used. Use eval_size instead.')

        # Starting filters for each perceptron:
        self.nodes_partition = [np.sum(list_boost_filters[:i]) for i in range(len(list_boost_filters)+1)]

        customBatchIterator = BatchIterator_Greedy(
            layer_output_shape=self.layer_output_shape,
            batch_size=self.batch_size,
            shuffle=self.batchShuffle,
        )

        # ---------------------------------------------------
        # Adjust the layer parameters:
        # ---------------------------------------------------
        self.layer_kwargs = deepcopy(layer_kwargs)
        self.layer_kwargs['name'] = 'greedyConv_1'



        # -------------------------------------
        # CONSTRUCT NETWORK:
        # -------------------------------------
        # tot_num_filters = np.sum(list_boost_filters)
        self.layer_type = layers_info[name_trained_layer]['type']
        netLayers = deepcopy(fixed_input_layers)
        if self.layer_type=="conv":
            # Here it should be modified to take all the parameters from
            #
            netLayers += [
                (layers.Conv2DLayer, self.layer_kwargs),
                (MaskLayer,{
                    'name': 'mask',
                    'list_boost_filters': list_boost_filters,
                    'num_classes': self.num_classes
                    }),
                (layers.Conv2DLayer, {
                    'name': 'greedyConv_2',
                    'num_filters': self.num_classes,
                    'filter_size': self.filter_size,
                    'pad':'same',
                    'W': Normal(std=self.init_weight),
                    'nonlinearity': segm_utils.softmax_segm})]

        elif self.layer_type=="trans_conv":
            self.layer_kwargs.setdefault('crop', 0)
            netLayers += [
                (layers.TrasposedConv2DLayer, self.layer_kwargs),
                (MaskLayer,{
                    'name': 'mask',
                    'list_boost_filters': list_boost_filters,
                    'num_classes': self.num_classes
                    }),
                (layers.TrasposedConv2DLayer, {
                    'name': 'greedyConv_2',
                    'num_filters': self.num_classes,
                    'filter_size': self.filter_size,
                    'crop': self.layer_kwargs['crop'],
                    'W': Normal(std=self.init_weight),
                    'nonlinearity': segm_utils.softmax_segm})]

        self.net = segmNeuralNet(
            layers=netLayers,
            batch_iterator_train = customBatchIterator,
            batch_iterator_test = customBatchIterator,
            objective_loss_function = segm_utils.categorical_crossentropy_segm,
            scores_train = [('trn pixelAcc', segm_utils.pixel_accuracy)],
            # scores_valid = [('val pixelAcc', pixel_accuracy)],
            y_tensor_type = T.ltensor3,
            eval_size=self.eval_size,
            regression = False,
            **greedy_kwargs
        )

        self.net._output_layer = self.net.initialize_layers()


        # Set all fixed layers as not trainable & reg:
        fixed_layers_names = [layer[1]['name'] for layer in fixed_input_layers]

        for name in fixed_layers_names:
            if layers_info[name]['type']=='conv' or layers_info[name]['type']=='trans_conv':
                self.net.layers_[name].params[self.net.layers_[name].W].remove('trainable')
                self.net.layers_[name].params[self.net.layers_[name].b].remove('trainable')
                self.net.layers_[name].params[self.net.layers_[name].W].remove('regularizable')


        # Set the first greedy-layer as not trainable:
        self.net.layers_['greedyConv_1'].params[self.net.layers_['greedyConv_1'].W].remove('trainable')
        self.net.layers_['greedyConv_1'].params[self.net.layers_['greedyConv_1'].b].remove('trainable')

        # Set both greedy-layers as not regularizable:
        self.net.layers_['greedyConv_1'].params[self.net.layers_['greedyConv_1'].W].remove('regularizable')
        self.net.layers_['greedyConv_2'].params[self.net.layers_['greedyConv_2'].W].remove('regularizable')
        # tick = time.time()
        self.net.initialize()
        # tock = time.time()
        # print "Done! (%f sec.)\n\n\n" %(tock-tick)

        self.insert_weights_fixedLayers(fixed_weights)


        # -------------------------------------
        # SAVE INFO NET: (for log)
        # -------------------------------------
        # info['num_classes'] = self.num_classes
        # info.pop('update', None)
        # info.pop('on_epoch_finished', None)
        # info.pop('on_batch_finished', None)
        # info.pop('on_training_finished', None)
        # info.pop('noReg_loss', None)
        # for key in [key for key in info if 'update_' in key]:
        #     info[key] = info[key].get_value().item()
        # json.dump(info, file(info['logs_path']+'/info-net.txt', 'w'))

    def insert_weights_fixedLayers(self, fixed_weights):
        '''
        The function inserts the weights

        fixed_weights
            type: dictionary
            value: key->layer_name; content -> list of layer_paramsValues
        '''
        for layer in self.net.layers_:
            if layer not in ["greedyConv_1", "greedyConv_2", "mask"]:
                params_fixed_layer = self.net.layers_[layer].get_params()
                if layer in fixed_weights:
                    for i, param in enumerate(params_fixed_layer):
                        param.set_value(fixed_weights[layer][i])
                elif len(params_fixed_layer)!=0:
                    raise ValueError("Weights of fixed layer not available!")


    def insert_weights(self, boostedPerceptron):
        '''
        In order the following operations are done:
         - Update mask main part: activate another perceptron
         - Copy the boostedPerceptron weights in the greedyLayer

        Structure of parameters: (check lasagne doc)
         - W1: (num_classes*num_filters1*num_nodes, num_inputs, filter_length1)
         - b1: (num_classes*num_filters1*num_nodes, )
         - W2: (num_classes, num_classes*num_filters1*num_nodes, filter_length2)
         - b2: (num_classes,)
        '''

        # ------------------
        # Update mask:
        # ------------------
        self.net.layers_['mask'].add_perceptron()
        self.active_perceptrons = self.net.layers_['mask'].active_perceptrons

        # ------------------
        # Get weights:
        # ------------------
        all_net_params = layers.get_all_param_values(self.net.layers_['greedyConv_2'])
        W1, b1, maskParam, W2, b2 = all_net_params[-5:]
        perc_W1, perc_b1, perc_W2, perc_b2 = layers.get_all_param_values(boostedPerceptron.net.layers_['greedyConv_2'])[-4:]

        # --------------------
        # Update main part:
        # --------------------
        start = self.nodes_partition[self.active_perceptrons-1]
        stop = self.nodes_partition[self.active_perceptrons]
        slice_weights = slice(start,stop)
        # !!! For the moment I don't touch b2... !!! #
        b1[slice_weights] = perc_b1
        if self.layer_type=="conv":
            W1[slice_weights,:,:] = perc_W1
            W2[:,slice_weights,:] = perc_W2
        if self.layer_type=="trans_conv":
            W1[:,slice_weights,:] = perc_W1
            W2[slice_weights,:,:] = perc_W2
        layers.set_all_param_values(self.net.layers_['greedyConv_2'], all_net_params[:-5] + [W1, b1, maskParam, W2, b2])


    def get_greedy_weights(self):
        params_values = {}
        for layer_name in ['greedyConv_1', 'greedyConv_2', 'mask']:
            params = self.net.layers_[layer_name].get_params()
            params_values[layer_name] = [param.get_value() for param in params]
        return params_values

    def restore_greedy_weights(self, params_values):
        '''
        params_values:
            dictionary returned by get_greedy_weights
        '''
        for layer_name in ['greedyConv_1', 'greedyConv_2', 'mask']:
            params = self.net.layers_[layer_name].get_params()
            for i, param in enumerate(params):
                param.set_value(params_values[layer_name][i])


class MaskLayer(layers.Layer):
    '''
    --------------------------
    Subclass of lasagne.layers.Layer:
    --------------------------

    The received input should be in the form: (N, num_classes*num_nodes, dim_x, dim_y)

    Inputs:
     - num_filters1 (5)
     - num_classes (1)

    The only parameter of the layer is a 2-dim array containing the slice extremes
    deciding the active nodes. When initialized, no nodes are active.
    '''

    def __init__(self, incoming, *args, **kwargs):
        self.num_classes = kwargs.pop('num_classes', 2)
        self.list_boost_filters = kwargs.pop('list_boost_filters', None)
        if self.list_boost_filters is None:
            raise ValueError("List of filters not passed for boosting training")

        super(MaskLayer, self).__init__(incoming, *args, **kwargs)
        self.active_perceptrons = 0
        self.active_nodes = self.add_param(np.ones(1, dtype=np.int32), (1,), name='active_nodes', trainable=False, regularizable=False)
        #
        self.active_nodes.set_value([0])


    def add_perceptron(self):
        '''
        Add one node.

        The first time nothing is done (just add new node, but still no active
            ones in the main net)
        '''
        self.active_perceptrons += 1
        self.active_nodes.set_value([self.list_boost_filters[:self.active_perceptrons].sum()])


    def get_output_for(self, input, **kwargs):
        # Set to zero all the not-active perceptrons: (avoid backprop)
        return T.set_subtensor(input[:,self.active_nodes[0]:,:,:], 0.)



